# 项目名称

## 代码核心功能说明

### 算法基础

本项目采用 **多项式朴素贝叶斯分类器** 进行邮件分类任务。朴素贝叶斯分类器基于条件概率的特征独立性假设，假设在给定类别条件下，各特征之间是相互独立的。

在邮件分类任务中，贝叶斯定理的应用形式如下：

\[
P(C|X) = \frac{P(X|C)P(C)}{P(X)}
\]

其中：
- \(P(C|X)\) 表示在给定特征 \(X\) 的条件下，样本属于类别 \(C\) 的后验概率。
- \(P(X|C)\) 表示在类别 \(C\) 下特征 \(X\) 的条件概率。
- \(P(C)\) 是类别 \(C\) 的先验概率。
- \(P(X)\) 是特征 \(X\) 的边际概率，通常不依赖于类别，因此在分类过程中可以忽略。

具体到邮件分类的应用，特征 \(X\) 是邮件中的单词，类别 \(C\) 则是邮件的标签（如垃圾邮件与非垃圾邮件）。贝叶斯分类器根据训练数据中每个类别的先验概率以及给定类别下单词的条件概率，计算邮件属于某一类别的后验概率，从而完成分类。

### 数据处理流程

在本项目中，数据处理包括以下几个步骤：

1. **分词处理**：为了处理邮件中的文本信息，我们首先对文本进行分词处理，将邮件内容拆解成单个的词汇。分词是自然语言处理中的重要步骤，它将文本转化为机器可以理解的特征。

2. **停用词过滤**：在进行文本分析时，某些常见的词汇（如“的”，“了”，“在”等）对分类并没有实际意义，这些词被称为停用词。我们通过停用词表过滤掉这些无关的词，确保训练过程不会受到这些无效信息的干扰。

### 特征构建过程

在特征构建过程中，我们主要使用了两种常见的方法：**高频词特征选择**和**TF-IDF特征加权**。它们的区别如下：

1. **高频词特征选择**：
   高频词特征选择是根据词汇在整个语料库中的出现频率来进行特征选择的。选取出现频率较高的词汇作为特征。这种方法的一个问题是它没有考虑词汇在不同文档中的重要性，仅仅依赖于词汇的出现频率。

   数学表达：
   \[
   f_{\text{high\_freq}}(w) = \text{频率}(w)
   \]
   其中 \(w\) 是单词，频率表示单词 \(w\) 在训练集中的出现次数。

2. **TF-IDF特征加权**：
   TF-IDF（词频-逆文档频率）是一种衡量单词重要性的方法，它不仅考虑了词汇在当前文档中的出现频率（TF），还考虑了该词汇在整个语料库中的出现情况。TF-IDF能够减少在多个文档中频繁出现的常见词对分类的影响，强调那些在特定文档中频繁出现但在其他文档中出现较少的词汇。

   数学表达：
   \[
   \text{TF-IDF}(w, D) = \text{TF}(w, D) \times \log \left(\frac{N}{\text{DF}(w)}\right)
   \]
   其中：
   - \( \text{TF}(w, D) \) 是词汇 \(w\) 在文档 \(D\) 中的词频。
   - \( \text{DF}(w) \) 是包含词汇 \(w\) 的文档数。
   - \( N \) 是总文档数。
   
   **区别与实现差异**：
   - 高频词特征选择只考虑单词在语料库中出现的频率，而TF-IDF则考虑了词汇在特定文档中的重要性，并通过逆文档频率的方式降低常见词的权重。
   - 实现上，高频词特征选择可以直接通过计算单词频率实现，而TF-IDF需要计算每个单词在每个文档中的词频以及该单词在整个语料库中的逆文档频率。

通过这两种特征构建方法，可以为后续的分类算法提供更合适的特征，从而提升模型的准确性和鲁棒性。



<img src="https://github.com/power-Gz/dev_skills/blob/master/nlp-bayes/pic/1.png" width="800" alt="截图">

[//]: # (https://raw.githubusercontent.com/power-Gz/dev_skills/main/nlp-bayes/pic/1.png)